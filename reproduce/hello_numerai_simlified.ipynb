{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize NumerAPI - the official Python API client for Numerai\n",
    "from numerapi import NumerAPI\n",
    "import json\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import cloudpickle\n",
    "\n",
    "# Configuration\n",
    "DATA_VERSION = \"v5.0\"\n",
    "FEATURE_SET_SIZE = \"medium\"\n",
    "\n",
    "# Initialize API client\n",
    "napi = NumerAPI()\n",
    "\n",
    "# List available datasets and versions\n",
    "all_datasets = napi.list_datasets()\n",
    "dataset_versions = list(set(d.split('/')[0] for d in all_datasets))\n",
    "\n",
    "# Print all files available for download for our version\n",
    "current_version_files = [f for f in all_datasets if f.startswith(DATA_VERSION)]\n",
    "\n",
    "# Download and load feature metadata\n",
    "napi.download_dataset(f\"{DATA_VERSION}/features.json\")\n",
    "feature_metadata = json.load(open(f\"{DATA_VERSION}/features.json\"))\n",
    "\n",
    "# Display feature set sizes\n",
    "feature_sets = feature_metadata[\"feature_sets\"]\n",
    "feature_set = feature_sets[FEATURE_SET_SIZE]\n",
    "\n",
    "# Download and load training data\n",
    "napi.download_dataset(f\"{DATA_VERSION}/train.parquet\")\n",
    "train = pd.read_parquet(\n",
    "    f\"{DATA_VERSION}/train.parquet\",\n",
    "    columns=[\"era\", \"target\"] + feature_set\n",
    ")\n",
    "\n",
    "# Downsample to every 4th era to reduce memory usage and speedup model training\n",
    "train = train[train[\"era\"].isin(train[\"era\"].unique()[::4])]\n",
    "\n",
    "# Define model\n",
    "model = lgb.LGBMRegressor(\n",
    "    n_estimators=2000,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=5,\n",
    "    num_leaves=2**5-1,\n",
    "    colsample_bytree=0.1\n",
    ")\n",
    "\n",
    "# Train model\n",
    "model.fit(\n",
    "    train[feature_set],\n",
    "    train[\"target\"]\n",
    ")\n",
    "\n",
    "# Download and load validation data\n",
    "napi.download_dataset(f\"{DATA_VERSION}/validation.parquet\")\n",
    "validation = pd.read_parquet(\n",
    "    f\"{DATA_VERSION}/validation.parquet\",\n",
    "    columns=[\"era\", \"data_type\", \"target\"] + feature_set\n",
    ")\n",
    "validation = validation[validation[\"data_type\"] == \"validation\"]\n",
    "del validation[\"data_type\"]\n",
    "\n",
    "# Downsample validation data\n",
    "validation = validation[validation[\"era\"].isin(validation[\"era\"].unique()[::4])]\n",
    "\n",
    "# Apply embargo to avoid data leakage\n",
    "last_train_era = int(train[\"era\"].unique()[-1])\n",
    "eras_to_embargo = [str(era).zfill(4) for era in [last_train_era + i for i in range(4)]]\n",
    "validation = validation[~validation[\"era\"].isin(eras_to_embargo)]\n",
    "\n",
    "# Generate validation predictions\n",
    "validation[\"prediction\"] = model.predict(validation[feature_set])\n",
    "\n",
    "# Install and import scoring tools\n",
    "!pip install -q --no-deps numerai-tools\n",
    "from numerai_tools.scoring import numerai_corr, correlation_contribution\n",
    "\n",
    "# Download and join meta_model for validation\n",
    "napi.download_dataset(f\"v4.3/meta_model.parquet\", round_num=842)\n",
    "validation[\"meta_model\"] = pd.read_parquet(\n",
    "    f\"v4.3/meta_model.parquet\"\n",
    ")[\"numerai_meta_model\"]\n",
    "\n",
    "# Calculate performance metrics\n",
    "per_era_corr = validation.groupby(\"era\").apply(\n",
    "    lambda x: numerai_corr(x[[\"prediction\"]].dropna(), x[\"target\"].dropna())\n",
    ")\n",
    "per_era_mmc = validation.dropna().groupby(\"era\").apply(\n",
    "    lambda x: correlation_contribution(x[[\"prediction\"]], x[\"meta_model\"], x[\"target\"])\n",
    ")\n",
    "\n",
    "# Plot correlation metrics\n",
    "per_era_corr.plot(\n",
    "    title=\"Validation CORR\",\n",
    "    kind=\"bar\",\n",
    "    figsize=(8, 4),\n",
    "    xticks=[],\n",
    "    legend=False,\n",
    "    snap=False\n",
    ")\n",
    "per_era_mmc.plot(\n",
    "    title=\"Validation MMC\",\n",
    "    kind=\"bar\",\n",
    "    figsize=(8, 4),\n",
    "    xticks=[],\n",
    "    legend=False,\n",
    "    snap=False\n",
    ")\n",
    "\n",
    "# Plot cumulative metrics\n",
    "per_era_corr.cumsum().plot(\n",
    "    title=\"Cumulative Validation CORR\",\n",
    "    kind=\"line\",\n",
    "    figsize=(8, 4),\n",
    "    legend=False\n",
    ")\n",
    "per_era_mmc.cumsum().plot(\n",
    "    title=\"Cumulative Validation MMC\",\n",
    "    kind=\"line\",\n",
    "    figsize=(8, 4),\n",
    "    legend=False\n",
    ")\n",
    "\n",
    "# Calculate summary statistics\n",
    "corr_mean = per_era_corr.mean()\n",
    "corr_std = per_era_corr.std(ddof=0)\n",
    "corr_sharpe = corr_mean / corr_std\n",
    "corr_max_drawdown = (per_era_corr.cumsum().expanding(min_periods=1).max() - per_era_corr.cumsum()).max()\n",
    "\n",
    "mmc_mean = per_era_mmc.mean()\n",
    "mmc_std = per_era_mmc.std(ddof=0)\n",
    "mmc_sharpe = mmc_mean / mmc_std\n",
    "mmc_max_drawdown = (per_era_mmc.cumsum().expanding(min_periods=1).max() - per_era_mmc.cumsum()).max()\n",
    "\n",
    "# Display performance summary\n",
    "summary_df = pd.DataFrame({\n",
    "    \"mean\": [corr_mean, mmc_mean],\n",
    "    \"std\": [corr_std, mmc_std],\n",
    "    \"sharpe\": [corr_sharpe, mmc_sharpe],\n",
    "    \"max_drawdown\": [corr_max_drawdown, mmc_max_drawdown]\n",
    "}, index=[\"CORR\", \"MMC\"]).T\n",
    "\n",
    "# Download and process live data\n",
    "napi.download_dataset(f\"{DATA_VERSION}/live.parquet\")\n",
    "live_features = pd.read_parquet(f\"{DATA_VERSION}/live.parquet\", columns=feature_set)\n",
    "live_predictions = model.predict(live_features[feature_set])\n",
    "\n",
    "# Define prediction pipeline function\n",
    "def predict(live_features: pd.DataFrame) -> pd.DataFrame:\n",
    "    live_predictions = model.predict(live_features[feature_set])\n",
    "    submission = pd.Series(live_predictions, index=live_features.index)\n",
    "    return submission.to_frame(\"prediction\")\n",
    "\n",
    "# Serialize prediction function\n",
    "p = cloudpickle.dumps(predict)\n",
    "with open(\"hello_numerai.pkl\", \"wb\") as f:\n",
    "    f.write(p)\n",
    "\n",
    "# Download file if running in Google Colab\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download('hello_numerai.pkl')\n",
    "except:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "numerai_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
